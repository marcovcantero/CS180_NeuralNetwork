{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62e1a5c-9359-4515-9c13-d4155884a11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\marco\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\marco\\anaconda3\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\marco\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\marco\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\marco\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.13.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.30)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (80.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\marco\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\marco\\appdata\\roaming\\python\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas matplotlib seaborn scikit-learn tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc4da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup and Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fccde35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "\n",
    "def load_glove_embeddings(glove_file=\"../glove.6B.100d.txt\"):\n",
    "    \"\"\"Load GloVe embeddings into a dictionary\"\"\"\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    print(f'Found {len(embeddings_index)} word vectors.')\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86ab524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1300, 2)\n",
      "Label distribution:\n",
      "label\n",
      "2    519\n",
      "0    300\n",
      "1    255\n",
      "3    164\n",
      "4     62\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load TCFD dataset for training and testing\n",
    "train_df = pd.read_json('../data/train.json1', lines=True)\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "print(f\"Dataset shape: {train_df.shape}\")\n",
    "print(f\"Label distribution:\\n{train_df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244abcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed text: scope optional scope that includes indirect emissions associated with the goods and services supply chain produced outside the organization included are emissions from the transport of products from our logistics centres to stores downstream performed by external logistics operators air land and sea transport as well as the emissions associated with electricity consumption in franchise stores\n",
      "Corresponding label: 1 -> 1\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "train_df['text_clean'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text_clean'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels and fit on training data\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label_encoded'] = label_encoder.fit_transform(train_df['label'])\n",
    "test_df['label_encoded'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "print(f\"Sample processed text: {train_df['text_clean'][0]}\")\n",
    "print(f\"Corresponding label: {train_df['label'][0]} -> {train_df['label_encoded'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2637e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7963\n",
      "Training data shape: (1300, 500)\n",
      "Test data shape: (200, 500)\n",
      "Training labels shape: (1300,)\n",
      "Test labels shape: (200,)\n",
      "Final training set shape: (1040, 500)\n",
      "Validation set shape: (260, 500)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and Sequence Preparation\n",
    "\n",
    "# Parameters\n",
    "MAX_WORDS = 10000  # Maximum number of words in vocabulary\n",
    "MAX_LEN = 500      # Maximum sequence length\n",
    "EMBEDDING_DIM = 100  # GloVe embedding dimension\n",
    "\n",
    "# Tokenize texts\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df['text_clean'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text_clean'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text_clean'])\n",
    "\n",
    "\n",
    "# Pad sequences to same length\n",
    "X_train_full = pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_test = pad_sequences(test_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Prepare labels\n",
    "# y = to_categorical(df['label_encoded'])\n",
    "y_train_full = train_df['label_encoded'].values\n",
    "y_test = test_df['label_encoded'].values\n",
    "\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Training data shape: {X_train_full.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train_full.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(f\"Final training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae970f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 7050/7963 words\n",
      "Embedding matrix shape: (7964, 100)\n"
     ]
    }
   ],
   "source": [
    "## Create Embedding Matrix\n",
    "\n",
    "def create_embedding_matrix(tokenizer, embeddings_index, max_words, embedding_dim):\n",
    "    \"\"\"Create embedding matrix from GloVe embeddings\"\"\"\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_words, len(word_index)) + 1\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    \n",
    "    found_words = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_words:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                found_words += 1\n",
    "    \n",
    "    print(f\"Found embeddings for {found_words}/{num_words-1} words\")\n",
    "    return embedding_matrix\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    tokenizer, embeddings_index, MAX_WORDS, EMBEDDING_DIM\n",
    ")\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48af947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          796400    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 100)          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 128)          38528     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 167, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 167, 128)          65664     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 56, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 56, 128)           82048     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 19, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 19, 128)           82048     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 19, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 7, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 7, 128)            82048     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 7, 128)            0         \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 3, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 384)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               49280     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,196,661\n",
      "Trainable params: 400,261\n",
      "Non-trainable params: 796,400\n",
      "_________________________________________________________________\n",
      "Tokenizer vocab size: 7963\n",
      "Embedding matrix shape: (7964, 100)\n",
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "## Build CNN Model\n",
    "\n",
    "def build_cnn_model(vocab_size, max_len, embedding_dim, embedding_matrix, num_classes):\n",
    "    \"\"\"Build CNN model with pre-trained GloVe embeddings\"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding layer with pre-trained GloVe weights\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,  # Use actual vocabulary size\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=max_len,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False # Freeze embeddings\n",
    "        ),\n",
    "        \n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=3, padding='same'),\n",
    "        # Dropout(0.3),  # Increased dropout\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=4, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=3, padding='same'),\n",
    "        #Dropout(0.3),  # Add back dropout\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=3, padding='same'),\n",
    "        #Dropout(0.3),  # Add back dropout\n",
    "\n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(pool_size=3, padding='same'),\n",
    "\n",
    "        # Another Conv + Dropout + Pool\n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(pool_size=3, padding='same'),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # Higher dropout before final layer\n",
    "        # Dense(64, activation='relu'),\n",
    "        # Dropout(0.4),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "num_classes = len(np.unique(train_df['label_encoded']))\n",
    "model = build_cnn_model(embedding_matrix.shape[0], MAX_LEN, EMBEDDING_DIM, embedding_matrix, num_classes)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n",
    "\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer.word_index))\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)\n",
    "#print(\"Max token in sequences:\", np.max(X_padded))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "# print(\"Unique classes:\", np.unique(df['label_encoded']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d7a0be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1040, 500)\n",
      "Test set shape: (200, 500)\n",
      "Epoch 1/20\n",
      "130/130 [==============================] - 3s 20ms/step - loss: 0.1140 - accuracy: 0.9423 - val_loss: 1.0609 - val_accuracy: 0.6462\n",
      "Epoch 2/20\n",
      "130/130 [==============================] - 3s 20ms/step - loss: 0.1040 - accuracy: 0.9510 - val_loss: 1.0289 - val_accuracy: 0.6577\n",
      "Epoch 3/20\n",
      "130/130 [==============================] - 3s 20ms/step - loss: 0.1235 - accuracy: 0.9346 - val_loss: 1.0543 - val_accuracy: 0.6462\n",
      "Epoch 4/20\n",
      "130/130 [==============================] - 3s 20ms/step - loss: 0.0895 - accuracy: 0.9596 - val_loss: 1.0711 - val_accuracy: 0.6269\n",
      "Epoch 5/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.1663 - accuracy: 0.9260 - val_loss: 1.2890 - val_accuracy: 0.5846\n",
      "Epoch 6/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.1070 - accuracy: 0.9567 - val_loss: 1.0216 - val_accuracy: 0.6577\n",
      "Epoch 7/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.0683 - accuracy: 0.9692 - val_loss: 1.0912 - val_accuracy: 0.6692\n",
      "Epoch 8/20\n",
      "130/130 [==============================] - 3s 22ms/step - loss: 0.0742 - accuracy: 0.9625 - val_loss: 1.2159 - val_accuracy: 0.6308\n",
      "Epoch 9/20\n",
      "130/130 [==============================] - 3s 20ms/step - loss: 0.0714 - accuracy: 0.9702 - val_loss: 1.5547 - val_accuracy: 0.5385\n",
      "Epoch 10/20\n",
      "130/130 [==============================] - 3s 23ms/step - loss: 0.1634 - accuracy: 0.9250 - val_loss: 1.0123 - val_accuracy: 0.6385\n",
      "Epoch 11/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.0601 - accuracy: 0.9788 - val_loss: 1.1519 - val_accuracy: 0.6500\n",
      "Epoch 12/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.0695 - accuracy: 0.9692 - val_loss: 1.1315 - val_accuracy: 0.6500\n",
      "Epoch 13/20\n",
      "130/130 [==============================] - 3s 20ms/step - loss: 0.0518 - accuracy: 0.9750 - val_loss: 1.1753 - val_accuracy: 0.6500\n",
      "Epoch 14/20\n",
      "130/130 [==============================] - 3s 20ms/step - loss: 0.0594 - accuracy: 0.9721 - val_loss: 1.1989 - val_accuracy: 0.6462\n",
      "Epoch 15/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.0565 - accuracy: 0.9721 - val_loss: 1.2191 - val_accuracy: 0.6154\n",
      "Epoch 16/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.0577 - accuracy: 0.9683 - val_loss: 1.2562 - val_accuracy: 0.6500\n",
      "Epoch 17/20\n",
      "130/130 [==============================] - 3s 20ms/step - loss: 0.0369 - accuracy: 0.9827 - val_loss: 1.2398 - val_accuracy: 0.6577\n",
      "Epoch 18/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.0534 - accuracy: 0.9760 - val_loss: 1.2832 - val_accuracy: 0.6346\n",
      "Epoch 19/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.1310 - accuracy: 0.9413 - val_loss: 1.2070 - val_accuracy: 0.6308\n",
      "Epoch 20/20\n",
      "130/130 [==============================] - 3s 21ms/step - loss: 0.0825 - accuracy: 0.9702 - val_loss: 1.1661 - val_accuracy: 0.6231\n"
     ]
    }
   ],
   "source": [
    "## Train the Model\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=8,\n",
    "    epochs=40,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "822af0c8-11ce-4f0d-9746-bf276e2eaa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "2    0.399038\n",
      "0    0.230769\n",
      "1    0.196154\n",
      "3    0.125962\n",
      "4    0.048077\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class weights: {0: 0.8666666666666667, 1: 1.0196078431372548, 2: 0.5012048192771085, 3: 1.5877862595419847, 4: 4.16}\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"\\nClass weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0a070f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 1.6331\n",
      "Test Accuracy: 0.5700\n",
      "Accuracy: 0.5700\n",
      "F1 Macro: 0.4590\n",
      "F1 Weighted: 0.5639\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "print(f\"F1 Weighted: {f1_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b261e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save('./cnn_model/cnn_model.h5')\n",
    "\n",
    "# tokenizer\n",
    "import pickle\n",
    "with open('./cnn_model/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# embedding matrix\n",
    "np.save('./cnn_model/embedding_matrix.npy', embedding_matrix)\n",
    "\n",
    "# model parameters/config\n",
    "model_config = {\n",
    "    'MAX_LEN': MAX_LEN,\n",
    "    'EMBEDDING_DIM': EMBEDDING_DIM,\n",
    "    'vocab_size': embedding_matrix.shape[0],\n",
    "    'num_classes': num_classes\n",
    "}\n",
    "\n",
    "with open('./cnn_model/model_config.pkl', 'wb') as f:\n",
    "    pickle.dump(model_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c94b25b-d45a-41f0-a65c-b57d67b3ae8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Call the function with class labels\u001b[39;00m\n\u001b[0;32m     25\u001b[0m class_names \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m---> 26\u001b[0m plot_conf_matrix(\u001b[43my_true\u001b[49m, y_pred_classes, class_names)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Predict on dev or test set\n",
    "# y_true = label_encoder.transform(test_df['label'])  # Actual labels\n",
    "# y_pred_proba = model.predict(X_dev)                # Raw model predictions\n",
    "# y_pred = np.argmax(y_pred_proba, axis=1)           # Convert to label indices\n",
    "\n",
    "# Confusion matrix plotting function\n",
    "def plot_conf_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with class labels\n",
    "class_names = label_encoder.classes_\n",
    "plot_conf_matrix(y_true, y_pred_classes, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53a65b-d5b8-4d0a-a126-2bbb8a635070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
